<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<sst xmlns="http://schemas.openxmlformats.org/spreadsheetml/2006/main" count="20806" uniqueCount="75"><si><t xml:space="preserve">longitude</t></si><si><t xml:space="preserve">latitude</t></si><si><t xml:space="preserve">housing_median_age</t></si><si><t xml:space="preserve">total_rooms</t></si><si><t xml:space="preserve">total_bedrooms</t></si><si><t xml:space="preserve">population</t></si><si><t xml:space="preserve">households</t></si><si><t xml:space="preserve">median_income</t></si><si><t xml:space="preserve">median_house_value</t></si><si><t xml:space="preserve">ocean_proximity</t></si><si><t xml:space="preserve">NEAR BAY</t></si><si><t xml:space="preserve">&lt;1H OCEAN</t></si><si><t xml:space="preserve">INLAND</t></si><si><t xml:space="preserve">NEAR OCEAN</t></si><si><t xml:space="preserve">ISLAND</t></si><si><t xml:space="preserve">0. to avoid replacing blank cells with 0s, let us fill in blanks using the string “$BLANK”</t></si><si><t xml:space="preserve">1. We shuffle by adding a random index (column A) using Sheet &gt; Fill Cells &gt; Fill Random number. Then we sort by column A ascending.</t></si><si><t xml:space="preserve">index</t></si><si><t xml:space="preserve">2. Now we split into testing data and training data (discarding the unnecessary random index column).</t></si><si><t xml:space="preserve">Testing data</t></si><si><t xml:space="preserve">Total count</t></si><si><t xml:space="preserve">Test size</t></si><si><t xml:space="preserve">Test count</t></si><si><t xml:space="preserve">First test row</t></si><si><t xml:space="preserve">Last test row</t></si><si><t xml:space="preserve">Training data</t></si><si><t xml:space="preserve">3. We categorize the features of the training data. The dependent feature is “median_house_value“.&#10;We further categorize the independent features as numerical or categorical. Categorical features are further yet divided into ordinal and nominal.&#10;Ordinal features can be assigned a hierarchical score starting with 0, where an increase in that score is an increase in the feature. (Such is the case with ocean_proximity.&#10;Nominal features cannot be assigned score; we may think of these as names.</t></si><si><t xml:space="preserve">numerical</t></si><si><t xml:space="preserve">ordinal</t></si><si><t xml:space="preserve">dependent</t></si><si><t xml:space="preserve">4.0 Before the categorical features can be manipulated arithmetically, they must be encoded as numbers.&#10;Until then, we can at least count the occurrences and the total count, which allows us to see the total number of each.</t></si><si><t xml:space="preserve">encoding</t></si><si><t xml:space="preserve">multiplicity</t></si><si><t xml:space="preserve">frequency</t></si><si><t xml:space="preserve">all</t></si><si><t xml:space="preserve">diff</t></si><si><t xml:space="preserve">4.1 Ordinal values are replaced with the assigned score. This is known as label encoding.</t></si><si><t xml:space="preserve">4.2 Nominal features are one-hot encoded. The trade-off is that we remove the risk of introducing a hierarchy by increasing the dimensionality of the data.&#10;In one-hot encoding, each of the categories becomes a new feature, assigned 1 if the original category matches the new feature or 0 otherwise, and the original feature is removed.&#10;The order of the new features does not matter because they are nominal, not ordinal.&#10;Also, we do this for all features except for one. Why? Because that last feature is not linearly independent from the rest, because one can calculate it as (1 - sum(other features)).&#10;Just be sure to document this as with everything.&#10;&#10;In our case, we did not have any nominal features.&#10;However, for example, assume we have data that includes fruits. Fruits cannot be scored objectively by this data. Thus, we use one-hot encoding.&#10;In this example of the one-hot encoding, we use alphabetical ordering, leaving out the pear category, but it could have been any of the others. Pears are the […,0,0,0,...] rows.</t></si><si><t xml:space="preserve">original data</t></si><si><t xml:space="preserve">count categories</t></si><si><t xml:space="preserve">one hot encoding</t></si><si><t xml:space="preserve">…</t></si><si><t xml:space="preserve">nominal</t></si><si><t xml:space="preserve">fruit</t></si><si><t xml:space="preserve">apple</t></si><si><t xml:space="preserve">orange</t></si><si><t xml:space="preserve">peach</t></si><si><t xml:space="preserve">pear</t></si><si><t xml:space="preserve">5.0 Let us consider the relation of each encoded feature to the dependent feature by plotting the relation.</t></si><si><t xml:space="preserve">5.1 We notice that longitude, latitude both seem to be periodic,&#10;but these with ocean_proximity each have very little relationship to the median_house_value because in each (x, y) relation, there may be many y values for each x.&#10;For total_rooms, total_bedrooms, population, households, median_income, we notice more settling away from 0. However, the values are more sparse. So let’s apply x’ := ln(x + 1), remembering $BLANK.&#10;Since we will be applying a linear regression, the objective of this step is to make each relation more linear, at least on average.</t></si><si><t xml:space="preserve">4.0 We describe numerical data statistically.</t></si><si><t xml:space="preserve">categorical</t></si><si><t xml:space="preserve">percentile</t></si><si><t xml:space="preserve">count</t></si><si><t xml:space="preserve">mean</t></si><si><t xml:space="preserve">stdev</t></si><si><t xml:space="preserve">min</t></si><si><t xml:space="preserve">low quartile</t></si><si><t xml:space="preserve">median</t></si><si><t xml:space="preserve">upper quartile</t></si><si><t xml:space="preserve">max</t></si><si><t xml:space="preserve">IQR</t></si><si><t xml:space="preserve">range</t></si><si><t xml:space="preserve">4.1 Minimal non-zero central interpercentile range:&#10;For each feature, starting at the lower percentile with percent p = 25% and the matching upper percentile with percent (1 – p), we find the first non-zero range.&#10;Then scale by dividing by (2 – 4p).&#10;The interquartile range is referenced for comparison.</t></si><si><t xml:space="preserve">percent</t></si><si><t xml:space="preserve">lower percentile</t></si><si><t xml:space="preserve">upper percentile</t></si><si><t xml:space="preserve">nonzero range</t></si><si><t xml:space="preserve">inter percentile range</t></si><si><t xml:space="preserve">scaled</t></si><si><t xml:space="preserve">ref IQR</t></si><si><t xml:space="preserve">4.2 for the number of histogram bins, we use the Freedman-Diaconis’s choice formula</t></si><si><t xml:space="preserve">scaled IPR</t></si><si><t xml:space="preserve">bin width</t></si><si><t xml:space="preserve"># bins</t></si></sst>