<?xml version="1.0" encoding="UTF-8" standalone="yes"?>
<sst xmlns="http://schemas.openxmlformats.org/spreadsheetml/2006/main" count="16996" uniqueCount="129"><si><t xml:space="preserve">Every change we make to the training data while cleaning it, must be made to the testing data as&#10;well and in the same order, so we record every change in this summary.</t></si><si><t xml:space="preserve">change</t></si><si><t xml:space="preserve">param #0 name</t></si><si><t xml:space="preserve">param #0 value</t></si><si><t xml:space="preserve">param #1 name</t></si><si><t xml:space="preserve">param #1 value</t></si><si><t xml:space="preserve">encoding</t></si><si><t xml:space="preserve">replace missing</t></si><si><t xml:space="preserve">longitude</t></si><si><t xml:space="preserve">ocean_proximity</t></si><si><t xml:space="preserve">&lt;1H OCEAN</t></si><si><t xml:space="preserve">INLAND</t></si><si><t xml:space="preserve">ISLAND</t></si><si><t xml:space="preserve">NEAR BAY</t></si><si><t xml:space="preserve">latitude</t></si><si><t xml:space="preserve">housing_median_age</t></si><si><t xml:space="preserve">total_rooms</t></si><si><t xml:space="preserve">total_bedrooms</t></si><si><t xml:space="preserve">population</t></si><si><t xml:space="preserve">NEAR OCEAN</t></si><si><t xml:space="preserve">households</t></si><si><t xml:space="preserve">median_income</t></si><si><t xml:space="preserve">xy split</t></si><si><t xml:space="preserve">label</t></si><si><t xml:space="preserve">median_house_value</t></si><si><t xml:space="preserve">one hot encoding</t></si><si><t xml:space="preserve">see I3:M10</t></si><si><t xml:space="preserve">normalization</t></si><si><t xml:space="preserve">center longitude</t></si><si><t xml:space="preserve">scale longitude</t></si><si><t xml:space="preserve">center latitude</t></si><si><t xml:space="preserve">scale latitude</t></si><si><t xml:space="preserve">center housing_median_age</t></si><si><t xml:space="preserve">scale housing_median_age</t></si><si><t xml:space="preserve">center total_rooms</t></si><si><t xml:space="preserve">scale total_rooms</t></si><si><t xml:space="preserve">center total_bedrooms</t></si><si><t xml:space="preserve">scale total_bedrooms</t></si><si><t xml:space="preserve">center population</t></si><si><t xml:space="preserve">scale population</t></si><si><t xml:space="preserve">center households</t></si><si><t xml:space="preserve">scale households</t></si><si><t xml:space="preserve">center median_income</t></si><si><t xml:space="preserve">scale median_income</t></si><si><t xml:space="preserve">center &lt;1H OCEAN</t></si><si><t xml:space="preserve">scale &lt;1H OCEAN</t></si><si><t xml:space="preserve">center INLAND</t></si><si><t xml:space="preserve">scale INLAND</t></si><si><t xml:space="preserve">center ISLAND</t></si><si><t xml:space="preserve">scale ISLAND</t></si><si><t xml:space="preserve">center NEAR BAY</t></si><si><t xml:space="preserve">scale NEAR BAY</t></si><si><t xml:space="preserve">We copy the data from the &quot;split training&quot; spreadsheet in the &quot;01-split_data.xlsx&quot; workbook.</t></si><si><t xml:space="preserve">original empty</t></si><si><t xml:space="preserve">#N/A</t></si><si><t xml:space="preserve">training count</t></si><si><t xml:space="preserve">Identify the features (independent variables) and label (dependent variable). We are analyzing&#10;longitude, latitude, housing median age, total number of rooms, total number of bedrooms,&#10;population, number of households, median income, and ocean proximity to predict the median&#10;house value. Accordingly, let’s categorize and set apart the features and the label.</t></si><si><t xml:space="preserve">features</t></si><si><t xml:space="preserve">Features may be further categorized as numerical (which uses numbers on which you can perform arithmetic for values) or categorical data (which uses names on which you cannot perform arithmetic for values). In order to apply a machine learning algorithm, all data must be numerical. The more general technique to convert from categorical data to convert from categorical to numerical is one-hot encoding.&#10;&#10;One-hot encoding is performed by transforming the column for that feature into a separate column for each&#10;category in the original feature, which is assigned the value &quot;1&quot; when the new feature matches the original &#10;category or &quot;0&quot; when it does not. Additionally, because for each row, the sum of these features always adds up to 1, they are linearly dependent, so one feature may be omitted as long as it is documented which was omitted. This feature may always be reconstituted as =(1 – SUM(&lt;other features&gt;)) as needed.&#10;&#10;Categorical data may be divided further yet into ordinal data (which can be formed into a ranking system) &#10;and nominal data (which cannot be). One-hot encoding is used for nominal data because it does not enforce&#10;a relationship between the different categories. However, when such a relationship is beneficial, which is the &#10;case for ordinal data, label encoding is used, which keeps the feature as 1 column, but gives each ordinal an &#10;integer value starting at 0 being the lowest according to the assigned ranking system. When there are only two categories, one-hot encoding and label encoding are one and the same. The only choice is which category to use for 0 (usually absence), and which for 1 (usually presence).&#10;&#10;Since both techniques involve replacing the value in the original data, it is a good time to replace any missing values in the categorical data. When replacing missing values, we aim to replace these values with the center. From statistics, we know that 3 of the measurements for center are the mean, median and mode. Since categorical data can only be one of a discrete set of specific values, there is no sense in using the mean or median which may attempt to give results between those values, which would be undefined. For this reason, we prefer the mode, which gives the most frequent value, as the center for categorical data.&#10;&#10;An alternative to replacing missing categorical data with the mode is the kNN (k nearest neighbors) algorithm. It has the advantage that it takes the other features into account to find the categorical values as labels. However, it is computationally expensive and outside of the scope of this lesson as it is a more advanced algorithm than linear regression.&#10;&#10;In this case, we could benefit better results from label encoding, but I will use one-hot encoding because it &#10;requires more explanation, but is also more general, so we can use it when we are unsure. The trade off of using one-hot encoding is the positive of removing the risk of introducing rating where there may be none, for the negative of higher dimensionality (more features features to clean and analyze). So when we are sure if the data is ordinal or nominal, prefer label encoding for ordinal data and one-hot for nominal data.</t></si><si><t xml:space="preserve">Let’s split the data as numerical, ordinal, nominal.&#10;In this example, ocean proximity is ordinal, but we do not have nominal data. However, we will treat ocean proximity as nominal.&#10;At this point, we can find the mode by counting the multiplicity of each category of ocean_proximity.&#10;&#10;Use COUNTA to count any and all text in a range, and =COUNTIF(&lt;range&gt;,&lt;key&gt;) to count all values equal to &lt;key&gt; in the given &lt;range&gt;. Since COUNTA will count all text, including headers, I moved the headers into a new column inserted before the ocean proximity data, then I merged the header cells with those to their right and hid the column with the header cells, allowing me to operate on the data as a whole column (just a nice spreadsheet trick also useful for parameters).</t></si><si><t xml:space="preserve">numerical</t></si><si><t xml:space="preserve">ordinal</t></si><si><t xml:space="preserve">count categories</t></si><si><t xml:space="preserve">multiplicity</t></si><si><t xml:space="preserve">frequency</t></si><si><t xml:space="preserve">all</t></si><si><t xml:space="preserve">total diff</t></si><si><t xml:space="preserve">mode</t></si><si><t xml:space="preserve">Let’s replace every missing value (empty cell) in the training data with the value specified in the&#10;summary (copied from the &quot;categorize features&quot; spreadsheet for portability).&#10;Although missing values will be empty in real data, note that in this training data, it is marked&#10;&quot;#N/A&quot; as shown in cell N4.</t></si><si><t xml:space="preserve">Again, although ocean proximity is ordinal, we are treating ocean proximity as nominal and using one-hot encoding for this example.&#10;Let’s create the table for encoding.&#10;Since ocean proximity categories are &quot;nominal&quot; (or at least we’re treating them as such), the order of the categories does not matter, and I chose ASCII order. In &quot;categorize features&quot;, we identified each category and counted the occurrence thereof (the multiplicity), which allows us to see we have not missed any category. Notice how &quot;NEAR OCEAN&quot; is encoded as [0, 0, 0, 0].</t></si><si><t xml:space="preserve">Now let’s replace the ocean proximity column by using the one-hot encoding from the summary&#10;(which has been copied from the &quot;categorize features&quot; sheet for portability). (Included is also a category count and difference for convenience. COUNTIFS counts the 0 rows.)&#10;&#10;We use the =VLOOKUP(&lt;key&gt;,&lt;array&gt;,&lt;# columns&gt;,&lt;range lookup&gt;) function to lookup this&#10;row’s ocean_proximity as &lt;key&gt; on the one-hot encoding table for ocean proximity &lt;array&gt; in the&#10;summary. The &lt;# columns&gt; represents the number of columns in the array, which can be found by&#10;using COLUMNS(&lt;array&gt;), and for &lt;range lookup&gt;, we use 0 for exact matches.&#10;Finally, I use relative column indices for the array because an earlier column will never need to be&#10;searched for in a later column. (For example, you don&apos;t need to search in &quot;INLAND&quot; for &#10;&quot;&lt;1H OCEAN&quot;.</t></si><si><t xml:space="preserve">parameter</t></si><si><t xml:space="preserve">count after</t></si><si><t xml:space="preserve">count before</t></si><si><t xml:space="preserve">diff</t></si><si><t xml:space="preserve">0-rows after</t></si><si><r><rPr><b val="true"/><sz val="10"/><rFont val="Arial"/><family val="2"/><charset val="1"/></rPr><t xml:space="preserve">Example:</t></r><r><rPr><sz val="10"/><rFont val="Arial"/><family val="2"/><charset val="1"/></rPr><t xml:space="preserve"> Let’s argue that ocean_proximity is ordinal and demonstrate label encoding.&#10;This could be done with a little knowledge about our data. Let’s analyze. But you may know this from the data repository or intuitively by the categories of the feature itself.&#10;To the right, we filter out the longitude and latitude matching each of the ocean_proximity categories.&#10;We can use these to create a scatter plot of each of the datasets representing these categories with&#10;Longitude as x and latitude as y shown below in Fig. 1.&#10;Latitude, longitude and ocean proximity are really just related geographic information systems data,&#10;and Fig. 1 shows this. It’s a map of the state of California with the Pacific ocean bordering its&#10;Western and southern shores.&#10;We can thus produce the label encoding in column B with 0 having the lowest median ocean&#10;proximity and 4 having the highest, giving the order [INLAND; &lt;1H OCEAN; NEAR BAY;&#10;NEAR OCEAN; ISLAND].&#10;The longitude counts from the categories on this sheet are compared with those from the&#10;&quot;one-hot encoding table&quot; spreadsheet to ensure that all data points have been plotted.&#10;For label encoding, we simply would exchange the values in column A with those in column B in the&#10;original data. However, let’s continue using one-hot encoding.</t></r></si><si><t xml:space="preserve">label count</t></si><si><t xml:space="preserve">one-hot count</t></si><si><t xml:space="preserve">In order to handle missing numerical data and normalization in general, let’s analyze the distribution of the data visually. In preparation, let’s find the ranges for each numerical feature and decide appropriate bin sizes for a histogram. There are many methods to choose bin size. However, we will try two simple methods. First, bins up to 4 standard deviations from the mean. Second, we will choose&#10;&#10;For normalizing categorical data, simple bar graphs will do since we’re dealing with discrete categories.&#10;&#10;Labels do not need to have missing data handled because linear regression is supervised learning, so we may&#10;assume all labels are known (or zero otherwise). Additionally, the reason for normalizing will be discussed&#10;later and it will be clear why labels do not need to be normalized.</t></si><si><t xml:space="preserve">In order to fill missing values, we choose a center. Examples of a measure of the center are the&#10;mean, median and mode. We discussed earlier how the mode is a good choice for categorical data&#10;by representing the value that most shows up. Now here are filling missing numerical data.&#10;&#10;The mean can be thought of the representative value over all the data. However, the mean may be&#10;affected by outliers. We can check for outliers by looking at whether the data’s distribution has a&#10;long tail. If it does, that leaves the median as a center. The median is the data point, at which 50%&#10;of the data is lower than it and 50% of the data is higher than it.&#10;&#10;To check the distribution of the data, we create a histogram, which is like a bar graph, but since&#10;numerical data is not in categories, we choose ranges called bins of a uniform size called a width.&#10;There are many better ways to choose bin width, but for the sake of simplicity, we will choose nice&#10;round ranges with the total number of bins around the range of 6–15. To that end, we need to know&#10;the minimum, maximum and range of each feature.&#10;&#10;Note that we are not analyzing labels. This is because labels do not need to have missing data&#10;handled because linear regression is supervised learning, so we may assume all labels are known&#10;(or zero otherwise).</t></si><si><t xml:space="preserve">min, actual</t></si><si><t xml:space="preserve">max, actual</t></si><si><t xml:space="preserve">count</t></si><si><t xml:space="preserve">min chosen</t></si><si><t xml:space="preserve">max chosen</t></si><si><t xml:space="preserve">range, actual</t></si><si><t xml:space="preserve">range chosen</t></si><si><t xml:space="preserve">bin width</t></si><si><t xml:space="preserve"># bins</t></si><si><t xml:space="preserve">We use the bin parameters from the &quot;# bins, missing numerical&quot; to construct the bin maximums in&#10;the first column named after each feature.&#10;From these bins, we count the number of values of that feature that are at most the bin maximum&#10;giving the cumulative value. Then, each next cumulative value is subtract from each current&#10;cumulative value, giving the next multiplicity.&#10;And finally, the multiplicity is divided by the total count giving the frequency.</t></si><si><t xml:space="preserve">cumulative</t></si><si><t xml:space="preserve">global min</t></si><si><t xml:space="preserve">bin #</t></si><si><t xml:space="preserve">We plot each numerical feature’s bins frequency vs their maximum creating the histograms.&#10;From these histograms, we see that longitude, latitude and housing_median_age each have&#10;non-longtail distributions. So for these, we will use the mean to fill missing values. The remaining&#10;total_rooms, total_bedrooms, population, households, and median_income are longtail &#10;distributions. So for these, we will use median to fill missing values.&#10;It is important to be able to distinguish a longtail distribution from a non-longtail distribution.&#10;Though if it’s really difficult to tell, it is better to err on the side of longtail for missing value fills. Because a non-longtail distribution is closer to being symmetrical and so its median will approximate its mean.&#10;Since no other changes have yet to be made to numerical values, let’s place these at the beginning with missing categorical data filling to deal with missing data early.</t></si><si><t xml:space="preserve">center type</t></si><si><t xml:space="preserve">mean</t></si><si><t xml:space="preserve">median</t></si><si><t xml:space="preserve">center value</t></si><si><t xml:space="preserve">As with categorical values in &quot;fill missing categories&quot;, we replace the missing values.&#10;From the workbook &quot;01-split_data&quot;, we know to expect 16512 entities. However, in&#10;the spreadsheet &quot;# bins, missing numerical&quot;, we saw that &quot;total_bedrooms&quot; actually&#10;had a count of 16341, which is 171 fewer than expected and our first example of&#10;actual missing data. Let’s count it to make sure the values are filled. We find no missing data now.</t></si><si><t xml:space="preserve">The different scales and centers of the features may exaggerate the effects of some features in the model. To alleviate this, we give all the data a common scale and center by normalizing.&#10;&#10;One normalization technique that may come to mind called min-max normalization is performed by subtracting the minimum and dividing by the range to place each feature in [0, 1]. However, this technique is prone to outliers, and as we’ve seen by the tailness tests, we have outliers. If we did not, min-max normalization would be fine, as it would be if all our data was one-hot encoded categorical data. One-hot encoded data is already min-max normalized in fact.&#10;&#10;So now we will make histograms showing the frequency of data up to 4 standard deviations away from the mean.&#10;&#10;This helps us notice if the data approximates a normal distribution. A normal or Gaussian distribution (also frequently called a bell-shaped graph) is a symmetrical distribution with data clustered at the mean, which is also the median, and a spread of 1 standard deviation. When it is not clear to tell, it’s safer to err on the side of a feature not being in a normal distribution.&#10;&#10;This allows us to pick between two normalization techniques, the standardization (which centers by mean and scales by the standard deviation resulting in the Z score) and the robust scaler (which centers by the median and scales by the interquartile range). Standardization is a good normalization for normal distributions, which have very few outliers. However, outliers will still standardize as large values. So the robust scaler will work for distributions with many outliers, and the interquartile range will approximate a standard deviation with outliers removed. Thus the centering is the same (about the median) and the scaling is the same (approximating the standard deviation ignoring outliers) for both normalizations.&#10;&#10;We will use the data with no changes from &quot;applied one-hot encoding&quot; to find statistics of the original training data. We need to find the means and standard deviations, specifically the sampling standard deviations because we sampled the training data from the original data. Included are also the count (for bin data) and the median and interquartile ranges, which may be useful for the normalization.</t></si><si><t xml:space="preserve">stdev</t></si><si><t xml:space="preserve">percentiles</t></si><si><t xml:space="preserve">IQR</t></si><si><t xml:space="preserve">We use the bin parameters from the &quot;# bins, missing numerical&quot; to construct the bin maximums in&#10;the first column named after each feature.&#10;This time we number the bins for all numerical features from -2 to +3, rather than from 0 to a number of bins. This is to find the frequency of the bins ±3 standard deviations from the mean. As a result of that, the data in the bins need not encompass all 100% of the original data.</t></si><si><t xml:space="preserve">max z score</t></si><si><t xml:space="preserve">These histograms are taken with each bin representing a range of consecutive z-scores (ending with the number under the bin and starting with the previous number). Their frequencies are plotted with a line representing the normal distribution according to the empirical rule (that says that 68% of normally distributed data falls within 1 standard deviation, 95% within 2 standard deviations, and 99.7% under 3 standard deviations).&#10;By making this comparison, we can test if the data is normally distributed.&#10;As we can see, only housing_median_age is normally distributed. So we will use mean for centering and standard deviation for scaling. As for the remaining, longitude, latitude, total_rooms, total_bedrooms, population, households, median_income, these are not normally distributed. So we will use median for centering and interquartile range for scaling.</t></si><si><t xml:space="preserve">empirical</t></si><si><t xml:space="preserve">robust scaler</t></si><si><t xml:space="preserve">standardization</t></si><si><t xml:space="preserve">scale</t></si><si><t xml:space="preserve">In order to fill missing values, we choose a center. Examples of a measure of the center are the &#10;mean, median and mode. We discussed earlier how the mode is a good choice for categorical data &#10;by representing the value that most shows up. &#10;The mean can be thought of as a value that represents the data over all. However, the mean is &#10;affected by outliers in the data. We can check for outliers by looking at whether the data has a&#10;long tail. If it does, that leaves the median as a center. The median is the data point, at which 50% &#10;of the data is lower than it and 50% of the data is higher than it.  &#10;&#10;For data that has been one-hot encoded, let’s use standardization for normalization even though &#10;it’s not normally distributed. The reason is as follows.  &#10;&#10;Each one-hot encoded feature has data that is either 0 or 1. For the robust scaler, there are 4 cases &#10;with one-hot encoded data. &#10;(Case 1) The data is distributed 50% to 0 and 50% to 1. In this case, the data has a median of 0.5 &#10;and an interquartile range of 1. So it is centered at 0 and scaled to [–0.5, +0.5]. However, the data &#10;will rarely ever be matched 50:50 this way. &#10;(Case 2/3) The data is distributed with either 0 or 1 having between 50% and 75% of the data, but &#10;neither exactly. The median will then be 0 or 1 respectively, and the interquartile range will be &#10;(1 – 0 = 1). In (Case 2), the data will be unaffected, remaining in the range [0, 1]. In case 3, the &#10;data will be negated as [-1, 0], so its weight will also be negated. Neither of these makes a &#10;difference. &#10;(Case 4) The data is distributed with either 0 or 1 having at least 75% of the data. Thus the &#10;interquartile range will be (0 – 0 = 0) or (1 – 1 = 0) respectively, both 0, leaving the normalized data &#10;as undefined. &#10;Thus we have ruled out the robust scaler.&#10;A tentative alternative solution is to leave the one-hot encoded data as min-max normalized (see&#10;&quot;# bins, normalization&quot;) by making no change, and seeing if the feature is overweighted. If so, we should consider another normalization besides robust scaler and min-max normalization.&#10;&#10;Now when it comes to standardization, one-hot encoded data is in the Bernoulli distribution. We &#10;have the frequency for ocean_proximity from &quot;categorize features&quot;, which is synonymous with the &#10;proportion of each category, p. We can use it to calculate the mean, p, and the standard deviation &#10;SQRT(p(1 – p)).&#10;&#10;The category &quot;NEAR OCEAN&quot; is included here for completeness. However, one of the ocean_proximity categories was redundant in one-hot encoding (due to linear dependence), and we decided to drop &quot;NEAR OCEAN&quot;.</t></si><si><t xml:space="preserve">center</t></si><si><t xml:space="preserve">ocean proximity</t></si><si><t xml:space="preserve">proportion</t></si><si><r><rPr><b val="true"/><sz val="10"/><rFont val="Arial"/><family val="2"/><charset val="1"/></rPr><t xml:space="preserve">Example: </t></r><r><rPr><sz val="10"/><rFont val="Arial"/><family val="2"/><charset val="1"/></rPr><t xml:space="preserve">This is done by using the mean and standard deviation. Mean is calculated as the &#10;expectation E[X], where X is each label, and E[g(X)] = SUM(g(X)*frequency(X)). The standard &#10;deviation is calculated as SQRT(E[X^2] - (E[X])^2). These are then used to find the z-score, Z, of &#10;each label. Finally, we find PHI(Z) = 1/2*(1 + ERF(-ABS(Z)/SQRT(2))) giving the normal distribution.&#10;&#10;From the resulting bar graph and the corresponding line of the normal distribution, we can see that &#10;Ocean_proximity is not a normal distribution. Thus we would use the robust scaler.&#10;&#10;Since we have the frequencies, rather than finding the percentiles from the original data, we can&#10;calculate the cumulative frequency, the 25</t></r><r><rPr><vertAlign val="superscript"/><sz val="10"/><rFont val="Arial"/><family val="2"/><charset val="1"/></rPr><t xml:space="preserve">th</t></r><r><rPr><sz val="10"/><rFont val="Arial"/><family val="2"/><charset val="1"/></rPr><t xml:space="preserve"> percentile, the median and 75</t></r><r><rPr><vertAlign val="superscript"/><sz val="10"/><rFont val="Arial"/><family val="2"/><charset val="1"/></rPr><t xml:space="preserve">th</t></r><r><rPr><sz val="10"/><rFont val="Arial"/><family val="2"/><charset val="1"/></rPr><t xml:space="preserve"> percentile are when&#10;the cumulative frequency is at least 25%, 50% and 75% respectively. Using this check on each&#10;cumulative frequency and counting the number of false values gives us the label for each.&#10;&#10;We find a median of 1 (&quot;&lt;1H OCEAN&quot;) for centering and an interquartile range of 1 for scaling.&#10;&#10;Now, if it were the case that the IQR was 0, we would have several options for compromise. In no&#10;particular order, we could reconsider the label encoding that we chose (if this is even possible), we could switch to one-hot encoding, or use standardization.</t></r></si><si><t xml:space="preserve">color table</t></si><si><t xml:space="preserve">mean of sq</t></si><si><t xml:space="preserve">z-score</t></si><si><t xml:space="preserve">PHI</t></si><si><t xml:space="preserve">004586</t></si><si><t xml:space="preserve">ff420e</t></si><si><t xml:space="preserve">ffd320</t></si><si><t xml:space="preserve">579d1c</t></si><si><t xml:space="preserve">7e0021</t></si><si><t xml:space="preserve">use</t></si><si><t xml:space="preserve">params</t></si><si><t xml:space="preserve">percentile</t></si><si><t xml:space="preserve">We subtract the center, then divide by the scale as show in the &quot;summary&quot; (which were copied&#10;from &quot;histogram, normalization&quot;, &quot;normalization for one-hot&quot;, and &quot;(example) label encode norm&apos;z&apos;n&quot;&#10;if applicable for portability).&#10;&#10;Below the data, we have rows of medians, means, interquartile ranges and standard deviations. &#10;We expect medians near 0 and standard deviations near 1. For one-hot encoded values, the &#10;median is almost always the mode (and so not very useful to us), so instead we show the mean, which should also be near 0. &#10;For other features with many outliers, the standard deviation may be further away from 1&#10;depending on the outliers, so we also have the interquartile range which should also be near 1 (not for housing_median_age though, which we have determined to be normally distributed). &#10;These conditions are satisfied.</t></si></sst>